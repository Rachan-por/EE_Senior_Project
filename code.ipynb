{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "  \"\"\"\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  #test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  # test_dataloader = DataLoader(\n",
    "  #     test_data,\n",
    "  #     batch_size=batch_size,\n",
    "  #     shuffle=True,\n",
    "  #     num_workers=num_workers,\n",
    "  #     pin_memory=True,\n",
    "  # )\n",
    "\n",
    "  return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader = create_dataloaders(\n",
    "    \"/Users/rachan/Desktop/CelebAx2_train\", \n",
    "    \"/Users/rachan/Desktop/CelebAx2_train\", \n",
    "    transform,\n",
    "    32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd7d89781f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 128, 128]), torch.Size([32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 1 \n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], 3, out_dim)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  62438883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleUnet(\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPositionEmbeddings()\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (downs): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=1024, bias=True)\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (output): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 10\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "!python data_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  62438883\n",
      "SimpleUnet(\n",
      "  (time_mlp): Sequential(\n",
      "    (0): SinusoidalPositionEmbeddings()\n",
      "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downs): ModuleList(\n",
      "    (0): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=1024, bias=True)\n",
      "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (ups): ModuleList(\n",
      "    (0): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (time_mlp): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (transform): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!python model_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "!python loss_function.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion():\n",
    "    def __init__(self, timesteps: int, device:str):\n",
    "        self.T = timesteps\n",
    "        self.betas = torch.linspace(0.0001, 0.02, self.T)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(\n",
    "            self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(\n",
    "            1. - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * \\\n",
    "            (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "\n",
    "    def get_index_from_list(self, vals, t, x_shape):\n",
    "        \"\"\" \n",
    "        Returns a specific index t of a passed list of values vals\n",
    "        while considering the batch dimension.\n",
    "        \"\"\"\n",
    "        batch_size = t.shape[0]\n",
    "        out = vals.gather(-1, t.cpu())\n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "    def forward_diffusion_sample(self, x_0, t, device='cpu'):\n",
    "        \"\"\" \n",
    "        Takes an image and a timestep as input and \n",
    "        returns the noisy version of it\n",
    "        \"\"\"\n",
    "        print(device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = get_index_from_list(\n",
    "            df.sqrt_alphas_cumprod, t, x_0.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "                df.sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "            )\n",
    "            # mean + variance\n",
    "        return sqrt_alphas_cumprod_t * x_0.to(device) \\\n",
    "            + sqrt_one_minus_alphas_cumprod_t.to(device) * \\\n",
    "            noise.to(device), noise.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Diffusion object at 0x7fd7df270460>\n"
     ]
    }
   ],
   "source": [
    "df = Diffusion(timesteps=100, device='cpu')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.randn([3, 128, 128])\n",
    "t = torch.randint(0, T, (1,), device='cpu').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6509,  1.5814, -1.2953,  ...,  1.1174, -1.8700, -0.8699],\n",
       "          [-0.1046,  0.8800, -2.1652,  ..., -0.6017,  1.4224, -1.5678],\n",
       "          [-1.2751, -0.9727, -0.9866,  ..., -1.0991, -0.6591,  0.9050],\n",
       "          ...,\n",
       "          [-1.8942, -2.2021,  0.4821,  ...,  0.7815,  1.2127, -0.0481],\n",
       "          [ 0.3326,  1.5892,  1.2392,  ..., -0.1221, -0.4928,  0.9036],\n",
       "          [-0.0869,  0.6065, -0.8448,  ..., -0.6293, -0.7503,  0.7203]],\n",
       " \n",
       "         [[-0.9900,  0.0377, -0.1942,  ..., -0.1260,  1.5745, -1.4457],\n",
       "          [ 0.3763,  0.1199,  0.6197,  ..., -0.6600, -0.9085,  1.2543],\n",
       "          [-0.5720, -0.6608,  0.6289,  ...,  1.0766, -1.1316, -0.3396],\n",
       "          ...,\n",
       "          [-1.2903,  0.3768,  0.5196,  ..., -0.5940, -2.2943, -0.8634],\n",
       "          [-0.9143,  0.1486,  0.2058,  ..., -0.6821,  0.3741, -1.0009],\n",
       "          [-1.0774,  0.8922,  0.6868,  ..., -0.0298, -0.2245,  0.2329]],\n",
       " \n",
       "         [[ 1.7853, -0.3524, -1.5612,  ...,  0.0469, -1.2702,  1.3568],\n",
       "          [ 1.5059, -2.3284,  0.5117,  ..., -1.7333,  0.7684,  0.8250],\n",
       "          [-0.7816,  0.5095,  1.1847,  ..., -0.7213,  1.0124, -0.7323],\n",
       "          ...,\n",
       "          [-0.8309,  0.9630,  0.7398,  ...,  0.1264,  0.5780, -1.4043],\n",
       "          [ 1.7045, -1.4004,  0.9213,  ...,  0.5757, -0.8387, -0.3188],\n",
       "          [ 1.8208,  0.1168, -0.5569,  ...,  1.2330,  1.7394,  0.2159]]]),\n",
       " tensor([[[-0.1218, -1.2965,  0.7189,  ...,  1.8433, -0.4774,  0.4901],\n",
       "          [ 0.1340,  0.0575,  1.9582,  ...,  1.2328,  0.2553,  1.7113],\n",
       "          [ 1.2688,  0.1864,  1.0319,  ...,  1.3424,  1.1774, -1.1653],\n",
       "          ...,\n",
       "          [-0.0936, -1.8422,  0.8364,  ..., -0.9845,  0.1572,  0.5412],\n",
       "          [-2.2728,  0.5049, -0.2935,  ..., -0.0252, -0.7430, -0.7245],\n",
       "          [-1.3579,  0.1799,  0.2731,  ...,  0.6618,  0.8916, -0.0830]],\n",
       " \n",
       "         [[ 1.2348,  0.1197, -0.1171,  ...,  0.4718,  1.0595, -0.8827],\n",
       "          [ 0.6339,  0.5215, -0.7098,  ...,  1.3023, -0.2046, -0.6733],\n",
       "          [-0.4899, -0.3835,  1.4974,  ...,  0.1613,  0.4373, -0.3381],\n",
       "          ...,\n",
       "          [-1.6475, -1.3307,  0.6070,  ..., -1.5194, -0.8411, -1.9410],\n",
       "          [ 0.2317,  0.1326, -1.0444,  ...,  0.8028,  0.8392, -0.0558],\n",
       "          [-1.7729, -0.1298,  0.1571,  ..., -0.0287,  0.1152, -3.5158]],\n",
       " \n",
       "         [[ 0.8569, -0.7123, -0.4933,  ..., -0.1846,  0.1402, -1.9343],\n",
       "          [-0.1821,  1.0136,  0.3393,  ..., -1.0109, -0.8921,  0.4060],\n",
       "          [ 1.9793,  0.7936,  0.1276,  ..., -0.1972, -0.2849,  0.2831],\n",
       "          ...,\n",
       "          [-1.2007,  0.0109, -0.6558,  ...,  0.4766,  1.8374,  2.4764],\n",
       "          [-2.0140,  0.8554,  0.3418,  ..., -0.7609,  0.1970,  0.6711],\n",
       "          [-0.2479, -0.3924, -0.2335,  ...,  1.7470, -0.9234,  0.5364]]]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.forward_diffusion_sample(x_0, t, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9999, 0.9997, 0.9995, 0.9993, 0.9991, 0.9989, 0.9987, 0.9985, 0.9983,\n",
      "        0.9981, 0.9979, 0.9977, 0.9975, 0.9973, 0.9971, 0.9969, 0.9967, 0.9965,\n",
      "        0.9963, 0.9961, 0.9959, 0.9957, 0.9955, 0.9953, 0.9951, 0.9949, 0.9947,\n",
      "        0.9945, 0.9943, 0.9941, 0.9939, 0.9937, 0.9935, 0.9933, 0.9931, 0.9929,\n",
      "        0.9927, 0.9925, 0.9923, 0.9921, 0.9919, 0.9917, 0.9915, 0.9913, 0.9911,\n",
      "        0.9909, 0.9907, 0.9905, 0.9903, 0.9901, 0.9898, 0.9896, 0.9894, 0.9892,\n",
      "        0.9890, 0.9888, 0.9886, 0.9884, 0.9882, 0.9880, 0.9878, 0.9876, 0.9874,\n",
      "        0.9872, 0.9870, 0.9868, 0.9866, 0.9864, 0.9862, 0.9860, 0.9858, 0.9856,\n",
      "        0.9854, 0.9852, 0.9850, 0.9848, 0.9846, 0.9844, 0.9842, 0.9840, 0.9838,\n",
      "        0.9836, 0.9834, 0.9832, 0.9830, 0.9828, 0.9826, 0.9824, 0.9822, 0.9820,\n",
      "        0.9818, 0.9816, 0.9814, 0.9812, 0.9810, 0.9808, 0.9806, 0.9804, 0.9802,\n",
      "        0.9800])\n",
      "(tensor([[[ 0.2565,  0.1360, -0.5729,  ...,  0.7214, -0.5790, -0.5875],\n",
      "         [ 0.4977,  0.9400,  0.0958,  ...,  1.1603, -0.6582,  0.4262],\n",
      "         [-0.7198, -0.6248, -1.0087,  ...,  2.3653,  0.4102,  0.4448],\n",
      "         ...,\n",
      "         [-0.9821,  0.0706,  0.3266,  ...,  0.1737,  0.6248, -1.9814],\n",
      "         [-0.0642, -0.2444, -0.2169,  ..., -0.8075,  0.8122, -0.7161],\n",
      "         [-1.4461,  1.4154, -1.4111,  ..., -0.0102,  1.5552,  0.7547]],\n",
      "\n",
      "        [[ 0.6497, -1.7827, -0.3260,  ...,  0.2687, -1.8793,  0.8009],\n",
      "         [-0.1111, -0.6131, -2.2792,  ...,  1.3411,  1.3308, -0.3444],\n",
      "         [-1.5855,  0.4969, -0.2977,  ...,  0.9456, -0.1973,  1.1457],\n",
      "         ...,\n",
      "         [ 0.4263,  0.9291,  0.9728,  ..., -0.4412,  1.4744, -0.1864],\n",
      "         [-0.1461,  0.8336, -0.9196,  ..., -0.3061, -2.0602, -0.3787],\n",
      "         [ 1.4422, -0.5691,  0.4987,  ..., -0.0680, -1.6304, -0.1682]],\n",
      "\n",
      "        [[-0.2677, -1.1833,  0.2776,  ..., -0.0160, -0.7748, -1.0095],\n",
      "         [ 0.5034, -1.2110,  0.4828,  ...,  1.8587,  0.8621,  0.4333],\n",
      "         [-0.6628, -1.5209,  1.0124,  ...,  0.2649, -0.6172, -0.1516],\n",
      "         ...,\n",
      "         [ 0.0609,  1.3612, -0.1105,  ..., -0.3837,  0.9062,  0.0754],\n",
      "         [ 0.5106,  0.2593, -0.8788,  ..., -1.0696, -1.0378,  0.2232],\n",
      "         [-0.8334, -0.1009, -1.0786,  ..., -0.3345,  1.2220, -0.7573]]]), tensor([[[ 8.4949e-01,  1.7052e-01,  1.1356e+00,  ...,  2.5492e-01,\n",
      "           8.8232e-01, -3.8537e-01],\n",
      "         [-2.6481e-01,  9.7199e-01,  1.1455e+00,  ...,  6.6822e-02,\n",
      "           9.5909e-01,  9.0640e-01],\n",
      "         [ 9.3748e-01,  4.3682e-01, -1.2399e+00,  ...,  8.2462e-01,\n",
      "           1.1159e+00, -2.4580e+00],\n",
      "         ...,\n",
      "         [ 5.4913e-01, -1.4548e+00,  3.8039e-02,  ..., -1.1520e+00,\n",
      "           1.0886e+00,  6.6577e-01],\n",
      "         [-2.6100e-01,  1.7442e+00, -6.8547e-01,  ...,  8.4367e-01,\n",
      "           1.9758e+00,  1.0788e-01],\n",
      "         [ 3.4621e-01, -3.0891e-02,  6.1646e-01,  ...,  2.3000e-01,\n",
      "           1.6797e+00, -7.2622e-01]],\n",
      "\n",
      "        [[ 9.1329e-01, -1.3259e+00,  2.8058e-01,  ..., -1.2611e+00,\n",
      "          -1.0577e+00, -5.7316e-01],\n",
      "         [ 7.2210e-01,  4.9349e-01, -8.3484e-01,  ...,  6.3149e-02,\n",
      "           1.3126e-01, -1.4811e-01],\n",
      "         [-2.3587e-02, -9.8191e-02,  6.8695e-01,  ..., -1.8872e-01,\n",
      "           8.7789e-04, -2.7405e-02],\n",
      "         ...,\n",
      "         [ 7.0570e-01,  8.3633e-01, -5.3477e-02,  ..., -3.2569e-01,\n",
      "          -8.8705e-02, -5.1565e-01],\n",
      "         [-2.7919e-01,  5.7448e-01, -8.6409e-01,  ..., -6.9812e-01,\n",
      "          -8.4782e-01,  8.6338e-02],\n",
      "         [ 5.3466e-01,  4.5342e-01,  4.2707e-01,  ...,  8.8766e-01,\n",
      "           1.5001e+00, -5.1777e-01]],\n",
      "\n",
      "        [[ 9.6087e-01, -6.4175e-01,  1.0272e+00,  ...,  9.1015e-01,\n",
      "          -1.0773e+00,  3.7993e-01],\n",
      "         [ 2.6453e+00, -1.6061e+00, -6.4694e-01,  ...,  1.5693e+00,\n",
      "           1.8080e-01, -1.1350e+00],\n",
      "         [ 1.4634e+00, -1.9736e+00,  1.3292e+00,  ...,  7.6581e-01,\n",
      "          -1.4207e+00,  1.9568e+00],\n",
      "         ...,\n",
      "         [ 3.3335e-01, -7.4407e-01,  3.3951e-01,  ..., -7.1544e-01,\n",
      "          -2.1714e-01, -2.1150e-02],\n",
      "         [-1.8313e+00,  1.5628e-01, -2.4230e+00,  ..., -4.4097e-01,\n",
      "           2.2718e+00,  2.6204e-01],\n",
      "         [-8.7086e-01,  3.7074e-01, -1.7485e+00,  ..., -1.8896e-01,\n",
      "           6.7499e-01,  8.2705e-01]]]))\n"
     ]
    }
   ],
   "source": [
    "!python forward.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
